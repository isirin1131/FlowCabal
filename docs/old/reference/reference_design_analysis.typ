#set text(font: "New Computer Modern", size: 11pt)
#set page(margin: (x: 2.5cm, y: 2.5cm))
#set heading(numbering: "1.1")
#set par(justify: true)

#align(center)[
  #text(size: 20pt, weight: "bold")[Reference Design Analysis for FlowCabal]
  #v(0.5cm)
  #text(size: 12pt, fill: gray)[Which designs to adopt, why, and what value transfers]
  #v(0.3cm)
  #text(size: 10pt, fill: gray)[February 17, 2026]
]

#v(1cm)

= Overview

Three reference projects were analyzed for design patterns transferable to FlowCabal:

#table(
  columns: (auto, 1fr, auto),
  stroke: 0.5pt,
  inset: 8pt,
  [*Project*], [*Domain*], [*Relevance*],
  [OpenViking], [AI agent context management --- virtual filesystem, tiered information, hierarchical retrieval], [Direct],
  [OpenClaw Memory], [AI agent persistent memory --- temporal compression, distillation, bounded storage], [Structural],
  [Trellis], [AI dev workflow orchestration --- specification injection, quality gates, multi-agent hooks], [Analogous],
)

FlowCabal is a visual workflow editor for AI-assisted long-form writing. Its core challenge is: _how does an AI agent assemble the right context for generating chapter N of an 80,000-word novel?_ The reference projects each solve related but different problems. This document extracts the most fundamental designs, prioritized by how directly they address FlowCabal's core challenge.

Designs are grouped into three tiers:

- *Tier 1*: Designs that directly solve FlowCabal's central problems. Adopt fully.
- *Tier 2*: Designs that significantly improve subsystems. Adapt with modifications.
- *Tier 3*: Patterns worth internalizing as principles. Apply where natural.

#pagebreak()

= Tier 1: Adopt Fully

== Three-Tier Information Model (OpenViking: L0 / L1 / L2)

=== Original value

OpenViking stores every resource at three granularity levels:

#table(
  columns: (auto, auto, 1fr),
  stroke: 0.5pt,
  inset: 8pt,
  [*Level*], [*Name*], [*Content*],
  [L0], [Abstract], [One-sentence description (< 50 tokens)],
  [L1], [Overview], [Structural summary: section headings, key points (200--500 tokens)],
  [L2], [Full Content], [Complete original text],
)

This solved a fundamental problem in RAG systems: retrieving full documents is token-expensive, but retrieving only embeddings loses structural context. By materializing three tiers, OpenViking lets an agent _scan broadly at L0, narrow at L1, and deep-read at L2_ --- a progressive loading strategy that mirrors how humans skim a library.

The three tiers are generated _asynchronously_ after ingestion. An Observer component watches for new resources and triggers VLM-based summarization in a background queue. This means ingestion is fast (just store L2), and the semantic layers are built lazily.

=== Transfer to FlowCabal

This is the highest-value design for FlowCabal. Role A (Context Agent) must decide which parts of a 200-chapter manuscript are relevant to the current node. Without tiered information, Role A would need to either:

- Read everything (impossible: blows the context window), or
- Rely solely on embedding similarity (insufficient: misses structural relationships).

With L0/L1/L2, Role A's retrieval becomes:

```
1. Scan all chapter L0s (< 50 tokens each, ~200 chapters = ~10K tokens)
2. Read L1 of promising chapters (~5 chapters x 400 tokens = 2K tokens)
3. Deep-read L2 of the most relevant 1--2 chapters
4. Always include: entity files, style guide, world rules (these are small)
```

Total context budget: ~15--20K tokens instead of the full 500K+ manuscript.

*What changes:* In OpenViking, L0/L1/L2 are generated by a general-purpose VLM. In FlowCabal, the summaries should be generated by the Agent LLM and _should be regenerated when a chapter's curated output is updated_. The summarization is not a one-time ingestion step --- it's part of the curation pipeline.

*Concrete mapping:*

#table(
  columns: (auto, auto),
  stroke: 0.5pt,
  inset: 8pt,
  [*OpenViking virtual path*], [*FlowCabal application*],
  [`/manuscript/chapter-N/content.md`], [L2: curated output of chapter N],
  [`/manuscript/chapter-N/summary-paragraph.md`], [L1: paragraph summary],
  [`/manuscript/chapter-N/summary-sentence.md`], [L0: one-sentence abstract],
  [`/manuscript/chapter-N/entity-changes.json`], [Entity state delta after chapter N],
  [`/entities/characters/*.md`], [Accumulated character profiles],
  [`/summaries/arc-*.md`], [Multi-chapter narrative arc summaries],
  [`/meta/style-guide.md`], [Writing style constraints],
)

== Hierarchical Retrieval with Intent Analysis (OpenViking)

=== Original value

OpenViking does not use flat vector search. Its `HierarchicalRetriever` combines two strategies:

+ *Global vector search*: finds semantically similar resources across the entire store.
+ *Recursive directory traversal*: given a target directory URI, walks the tree structure, using L0/L1 summaries to decide which subtrees to explore deeper.

Before retrieval, an `IntentAnalyzer` classifies the query to determine _what kind of context is needed_ --- not just what's textually similar. This is the difference between "find passages about dragons" (similarity search) and "find context needed to write a dialogue scene between the protagonist and antagonist" (intent-driven retrieval).

The two-stage architecture (vector recall + rerank) further separates fast candidate retrieval from slow but accurate relevance scoring.

=== Transfer to FlowCabal

Role A's context retrieval is FlowCabal's most performance-critical operation. Every node execution begins with Role A deciding what context to inject. The hierarchical retrieval pattern maps directly:

```python
# Role A's retrieval pipeline
def get_context(node_id, node_config, project):
    # Step 1: Intent analysis
    intent = agent_llm.analyze_intent(node_config, project.meta)
    # e.g., "dialogue scene, chapter 12, characters: [protag, antag]"

    # Step 2: Deterministic includes (always relevant)
    ctx = [project.meta.style_guide, project.meta.outline]

    # Step 3: Entity lookup (structured, not semantic)
    for char in intent.characters:
        ctx.append(project.entities.get(char))

    # Step 4: Hierarchical manuscript search
    #   Scan all chapter L0s → read promising L1s → deep-read best L2s
    ctx.extend(hierarchical_retrieve(intent, project.manuscript))

    # Step 5: Arc summaries for broader narrative context
    ctx.extend(project.summaries.get_relevant_arcs(intent))

    return ctx
```

*What changes:* OpenViking's retrieval is general-purpose (any document type). FlowCabal's retrieval is domain-specific (fiction manuscript). This is an advantage: Role A can use structured knowledge about what "writing chapter N" requires (previous chapter, relevant character arcs, world rules) rather than relying solely on semantic similarity. The intent analysis becomes simpler and more reliable because the domain is constrained.

== Curated Persistence: Not All Information Deserves Storage (OpenClaw + FlowCabal v3)

=== Original value (OpenClaw)

OpenClaw's 4D validation test requires every candidate memory entry to simultaneously satisfy:

+ *Error prevention*: without this info, the agent would make a specific, identifiable mistake.
+ *Broad applicability*: the info applies across many future sessions, not just one task.
+ *Self-contained*: the entry is understandable without additional context.
+ *Non-duplicate*: the info doesn't already exist in the store.

Plus a *reverse check*: "what specific error would occur without this?" If no concrete answer, the entry is rejected.

This is not just a filter --- it's a _philosophy of persistence_. The system assumes all information is ephemeral by default, and only promotes the most valuable pieces to long-term storage.

=== Transfer to FlowCabal

FlowCabal already has this principle in its v3 design: only user-approved outputs enter SQLite and trigger OpenViking indexing. But the _mechanism_ of curation can be enriched by OpenClaw's validation framework.

When a user selects `output:persist`, the system should not just blindly store. Role C (Monitor) or a dedicated curation step should:

+ Verify the output is self-contained enough to be useful as future context.
+ Check for contradictions with existing curated content (e.g., a character's eye color changed between chapters without authorial intent).
+ Generate L0/L1 summaries immediately.
+ Update entity change records.

The 4D test adapted for FlowCabal:

#table(
  columns: (auto, 1fr),
  stroke: 0.5pt,
  inset: 8pt,
  [*Dimension*], [*FlowCabal interpretation*],
  [Error prevention], [Will this content prevent a continuity error in future chapters?],
  [Broad applicability], [Will this content be referenced by multiple future nodes?],
  [Self-contained], [Can this content be understood without reading the full execution context?],
  [Non-duplicate], [Does this content add new narrative information beyond what's already curated?],
)

*What changes:* OpenClaw validates _agent memories about the user_. FlowCabal validates _creative writing outputs_. The validation criteria shift from "what mistake would the agent make?" to "what continuity error would future generation produce?" The fundamental principle --- aggressive filtering preserves signal quality --- is identical.

== Specification Injection Over Memory (Trellis)

=== Original value

Trellis's core insight: rather than hoping the AI remembers instructions from earlier in the conversation, _inject all relevant specifications fresh at every agent call_. Three hooks implement this:

- *Session start hook*: injects global project context.
- *Subagent context hook*: intercepts every agent dispatch and injects task-specific specifications.
- *Quality control hook*: enforces standards before allowing the agent to stop.

Each agent call becomes _self-contained_ --- it has everything it needs in its prompt, regardless of conversation history. This eliminates context drift, the gradual degradation of instruction-following as conversations grow longer.

=== Transfer to FlowCabal

FlowCabal's three-layer prompt assembly is structurally identical to Trellis's injection pattern:

#table(
  columns: (auto, auto, auto),
  stroke: 0.5pt,
  inset: 8pt,
  [*Trellis*], [*FlowCabal*], [*Mechanism*],
  [Session start hook], [Layer 1: User's TextBlockList], [Static specification loaded at execution start],
  [Subagent context hook], [Layer 2: Role A context injection], [Dynamic context injected per-node],
  [Quality control hook], [Role C evaluation], [Post-execution quality gate],
)

The key principle to internalize: *Agent context injection must be ephemeral*. FlowCabal's design already specifies that "Agent does NOT modify persisted metadata --- context injection is ephemeral." This is exactly Trellis's pattern: inject fresh, never mutate the source.

*What this means practically:* When implementing Role A, the context injection should be a pure function: `(node_config, project_state) -> additional_context`. It reads project state but never writes to it. The only writes happen through the curation pipeline (user-approved outputs) or through Role B (workflow topology changes, also requiring user approval).

This also implies that *re-running a node should produce the same context injection* given the same project state. Context assembly is deterministic given its inputs, even though the LLM output is stochastic.

#pagebreak()

= Tier 2: Adapt with Modifications

== Virtual Filesystem as Knowledge Organization (OpenViking)

=== Original value

OpenViking organizes all context as a virtual filesystem with deterministic paths. Every resource has a `viking://` URI. The filesystem paradigm provides:

- *Predictable addressing*: agents can construct paths programmatically (e.g., `viking://resources/characters/{name}`).
- *Hierarchical organization*: directories group related resources, enabling tree-based retrieval.
- *Uniform interface*: `ls`, `read`, `stat`, `find` --- operations agents already understand intuitively.

This is more than a naming convention. The directory structure _encodes domain knowledge_ about how information relates. The `/meta`, `/entities`, `/manuscript`, `/summaries` hierarchy is itself a design decision about what categories of knowledge exist.

=== Transfer to FlowCabal

FlowCabal's OpenViking integration already defines a project structure:

```
/project
  /meta          -- outline, style guide, world rules
  /entities      -- characters, locations, plot threads
  /manuscript    -- chapter content + summaries
  /summaries     -- arc and full-work summaries
```

The value to retain is the _deterministic path convention_. Role A should be able to construct paths like `/entities/characters/protagonist` without searching. This makes context assembly partially rule-based (always include `/meta/style-guide.md`) and partially search-based (find relevant chapters via L0 scanning).

*What changes:* OpenViking uses AGFS (a Go-based filesystem server) as its storage backend. FlowCabal uses SQLite directly. The virtual paths are logical keys in SQLite tables, not actual filesystem paths. This simplifies deployment (no separate process) but loses AGFS's native filesystem semantics. The trade-off is appropriate --- FlowCabal is a single-user local application, not a multi-tenant server.

== Temporal Compression for Project Knowledge (OpenClaw)

=== Original value

OpenClaw's temporal compression pipeline: daily logs → weekly summaries → long-term MEMORY.md. Each layer is progressively more distilled. The 7-day retention window keeps recent data in full detail while older data is compressed into categorical summaries (Decision / Discovery / Preference / Task).

This solves the _unbounded growth problem_: without compression, knowledge stores grow linearly with time, eventually consuming the context window. Compression maintains a fixed budget for long-term knowledge.

=== Transfer to FlowCabal

FlowCabal's manuscript _already_ has a similar structure if we view it through a temporal lens:

#table(
  columns: (auto, auto, auto),
  stroke: 0.5pt,
  inset: 8pt,
  [*OpenClaw*], [*FlowCabal equivalent*], [*Mechanism*],
  [Daily logs], [Per-chapter full content (L2)], [Complete text, always available],
  [Weekly summaries], [Per-chapter summaries (L1)], [Condensed form for scanning],
  [MEMORY.md], [Arc summaries + entity profiles], [Highest-level project knowledge],
)

The transferable principle is *bounded context budgets*. When assembling context for a node, Role A should operate within a token budget:

```
Total budget: ~20K tokens
  /meta (always):        ~2K tokens
  Entity files (by need): ~3K tokens
  Chapter L0 scan:        ~3K tokens (all chapters)
  Chapter L1 reads:       ~4K tokens (top 5-8 chapters)
  Chapter L2 deep reads:  ~6K tokens (top 1-2 chapters)
  Arc summaries:          ~2K tokens
```

If the manuscript grows to 500 chapters, the L0 scan grows to ~15K tokens, which is too much. At that point, the system should switch to *arc-level L0 scanning first*, then chapter-level within the selected arc. This is the temporal compression principle applied to narrative structure.

*What changes:* OpenClaw compresses along the _time axis_ (old data gets compressed). FlowCabal compresses along the _narrative distance axis_ (chapters far from the current one get more compressed). The mechanism is different but the principle is identical: progressive summarization with bounded budgets.

== Quality Gates as Structural Enforcement (Trellis: Ralph Loop)

=== Original value

Trellis's Ralph Loop intercepts the Check agent's stop event and requires proof of verification before allowing completion. It supports both programmatic checks (run linting commands) and marker-based checks (require specific completion tokens in output). A hard limit of 5 iterations prevents infinite loops.

The key insight: quality enforcement should be _structural_ (impossible to bypass) rather than _advisory_ (instructions that might be forgotten).

=== Transfer to FlowCabal

Role C (Monitor) evaluates output quality and decides: approve / retry / flag-for-human. The Ralph Loop pattern maps directly:

```python
MAX_RETRIES = 3

for attempt in range(MAX_RETRIES):
    output = user_llm.generate(prompt)
    evaluation = role_c.evaluate(output, node_config, project_state)

    if evaluation.decision == 'approve':
        ws.push('node:completed', output, evaluation)
        break
    elif evaluation.decision == 'retry':
        prompt = adjust_prompt(prompt, evaluation.feedback)
        continue
    elif evaluation.decision == 'flag-human':
        ws.push('node:needs-human', output, evaluation)
        human_response = await ws.receive('human:decision')
        handle_human_decision(human_response)
        break
else:
    # Max retries exhausted
    ws.push('node:needs-human', output, {'reason': 'max retries exceeded'})
```

*What changes:* Trellis's quality checks are about _code correctness_ (linting, type checking). FlowCabal's quality checks are about _narrative quality_ (continuity, character voice, plot coherence). The checks are necessarily more subjective and rely on the Agent LLM's judgment rather than deterministic tools. The structural pattern (retry loop with hard cap + human fallback) transfers directly.

== Relation System for Entity Tracking (OpenViking)

=== Original value

OpenViking supports explicit relation links between resources. Relations are stored as `.relations.json` files within directories and are surfaced during retrieval as `RelatedContext`. This enables the agent to discover thematically or structurally related resources that might not share textual similarity.

=== Transfer to FlowCabal

Entity tracking is a critical concern for long-form fiction. Characters, locations, and plot threads form a web of relationships. The relation system maps directly:

```
/entities/characters/protagonist  --relates-to-->  /entities/locations/castle
/entities/characters/protagonist  --conflicts-with-->  /entities/characters/antagonist
/manuscript/chapter-05            --introduces-->  /entities/characters/mentor
/entities/plot-threads/revenge    --involves-->  /entities/characters/antagonist
```

When Role A assembles context for a chapter, it can follow relation links: "this chapter involves the protagonist → follow links to find the protagonist's related locations and conflicting characters → include those entity files."

*What changes:* OpenViking's relations are manually created via `client.link()`. In FlowCabal, relations should be _automatically maintained_ by Role A during curation. When a chapter is curated, Role A parses entity changes and updates the relation graph. This is the `entity-changes.json` file in the OpenViking project structure.

#pagebreak()

= Tier 3: Internalize as Principles

== Idempotent Operations (OpenClaw)

OpenClaw's Session ID prefix matching ensures no session is captured twice, even under cron retries or clock skew. The principle: *every automated pipeline step should be safe to re-run*.

For FlowCabal, this applies to the curation pipeline. Curating the same output twice should be a no-op. Regenerating summaries for an already-summarized chapter should produce the same result or update in place. The `output:persist` handler should check for existing entries before inserting.

== Separation of Parsing and Semantics (OpenViking)

OpenViking has a strict rule: parsers never call LLMs. Parsing (converting a document to structured content) and semantic processing (generating summaries, embeddings) are fully decoupled and run asynchronously.

For FlowCabal, this means the curation pipeline should separate:
1. _Storage_ (immediate): save the curated output to SQLite.
2. _Semantic processing_ (async): generate L0/L1 summaries, update entity records, rebuild embeddings.

The user should get instant confirmation (`output:persisted`) without waiting for summarization to complete. Summaries can be generated in a background task and are available for the next execution cycle.

== Provenance Tracking (OpenClaw + OpenViking)

Both systems maintain full provenance chains. OpenClaw: MEMORY.md entry → weekly summary → daily log → session. OpenViking: search result → resource URI → original content.

FlowCabal's v3 design already includes `contextSources: string[]` in the `node:completed` message. This should be extended to track not just _which_ resources were used but _why_ Role A selected them:

```typescript
contextSources: {
  uri: string;           // e.g., "/entities/characters/protagonist"
  reason: string;        // e.g., "character appears in scene"
  level: 'L0' | 'L1' | 'L2';  // which tier was read
}[]
```

This enables users to understand and debug the context assembly process: "why did the AI think this chapter was about X?"

== Dual-Write Reliability (OpenClaw)

OpenClaw captures important information both via the active agent (real-time) and via cron jobs (safety net). The principle: critical data should have two capture paths.

For FlowCabal, this means the runtime cache (Python memory) and the curated store (SQLite) should have clear lifecycle semantics. If the application crashes during execution, the runtime cache is lost --- but that's acceptable because only user-approved outputs should persist. However, the _workflow state_ (which nodes have completed, their outputs) should be checkpointed to SQLite periodically during long executions, providing crash recovery.

== Single Source of Truth with Derived Data (Trellis)

Trellis's `AI_TOOLS` registry defines platform data once and derives everything else. Combined with compile-time assertions, this prevents incomplete additions.

For FlowCabal, this applies to the `core/` type definitions. `NodeDefinition`, `WorkflowDefinition`, and `TextBlockList` are the SSOT for workflow structure. The Python backend should mirror these types exactly (via code generation or shared schema), and the WebSocket protocol should be derived from them. Any divergence between browser and backend type definitions will cause subtle bugs.

#pagebreak()

= Summary: Priority Map

#table(
  columns: (auto, auto, auto, auto),
  stroke: 0.5pt,
  inset: 8pt,
  [*Priority*], [*Design*], [*Source*], [*FlowCabal Target*],
  [1], [Three-tier information (L0/L1/L2)], [OpenViking], [OpenViking adapter, curation pipeline],
  [2], [Hierarchical retrieval + intent], [OpenViking], [Role A context assembly],
  [3], [Curated persistence philosophy], [OpenClaw], [output:persist pipeline, Role C],
  [4], [Specification injection (ephemeral)], [Trellis], [Three-layer prompt assembly],
  [5], [Virtual filesystem organization], [OpenViking], [SQLite-backed OpenViking paths],
  [6], [Bounded context budgets], [OpenClaw], [Role A token budget management],
  [7], [Quality gates with retry loop], [Trellis], [Role C retry + human fallback],
  [8], [Entity relation tracking], [OpenViking], [Automatic relation maintenance],
  [9], [Idempotent curation], [OpenClaw], [output:persist deduplication],
  [10], [Async semantic processing], [OpenViking], [Background L0/L1 generation],
  [11], [Provenance in context sources], [Both], [Extended contextSources metadata],
  [12], [Type SSOT across languages], [Trellis], [TS/Python type synchronization],
)

== Shared Fundamental Insight

All three reference projects converge on one principle: *aggressive, principled filtering is the key to sustainable AI knowledge management.* OpenClaw's 4D validation, OpenViking's three-tier progressive loading, and Trellis's specification injection all serve the same purpose --- ensuring the AI receives _the right information in the right amount at the right time_, not everything all at once.

For FlowCabal, this manifests as: only curated outputs enter the knowledge store, only relevant context enters the prompt, and only the right level of detail (L0/L1/L2) is loaded for each resource. The workflow DAG provides the structural backbone; the Agent system provides the intelligence to navigate that structure efficiently.

#v(2cm)
#align(center)[
  #line(length: 40%, stroke: 0.5pt + gray)
  #v(0.5cm)
  #text(size: 10pt, fill: gray)[
    Reference Design Analysis \
    FlowCabal Project --- February 17, 2026
  ]
]
